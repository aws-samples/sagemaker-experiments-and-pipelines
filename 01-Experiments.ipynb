{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Use SageMaker Experiments to track and compare multiple trials\n",
    "\n",
    "In this notebook, we aim to show how data-scientists can use SageMaker Experiments to keep track and organize their machine learning experimentation.\n",
    "\n",
    "### The Machine Learning problem\n",
    "\n",
    "We will be predicting house values for the California districts. The problem is a classic regression problem and we will use the [California Housing dataset](https://scikit-learn.org/stable/modules/generated/sklearn.datasets.fetch_california_housing.html\n",
    ") obtained from [Scikit-Learn](https://scikit-learn.org/stable/datasets/real_world.html), which was originally published in:\n",
    "> Pace, R. Kelley, and Ronald Barry. \"Sparse spatial auto-regressions.\" Statistics & Probability Letters 33.3 (1997): 291-297.\n",
    "\n",
    "The target variable is the house value for California districts, expressed in hundreds of thousands of dollars (`$100,000`).\n",
    "\n",
    "We will be using the [Amazon SageMaker built-in XGBoost algorithm](https://docs.aws.amazon.com/sagemaker/latest/dg/xgboost.html) to create the model because it does not require the use of a script to run the experiments. However, the use of Amazon SageMaker Experiments or Amazon SageMaker Pipelines is not affected by whether you use a different built-in algorithm, use Amazon SageMaker in script mode, or bring your own container."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Install required and/or update libraries\n",
    "\n",
    "At the time of writing, the `sagemaker` SDK version tested is `2.73.0`, while the `sagemaker-experiment` SDK library is `0.1.35`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "import subprocess\n",
    "subprocess.check_call([sys.executable, '-m', 'pip', 'install', '-Uq', 'pip'])\n",
    "subprocess.check_call([sys.executable, '-m', 'pip', 'install', 'sagemaker==2.73.0', '-Uq'])\n",
    "subprocess.check_call([sys.executable, '-m', 'pip', 'install', 'sagemaker-experiments==0.1.35', '-Uq'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Set up\n",
    "\n",
    "Let's start by specifying:\n",
    "\n",
    "* The S3 bucket and prefix that you want to use for training and model data. This should be within the same region as the notebook instance, training, and hosting.\n",
    "* The IAM role arn used to give training and hosting access to your data.\n",
    "* The experiment name as the logical entity to keep our tests grouped and organized."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import boto3\n",
    "import sagemaker\n",
    "import time\n",
    "from time import strftime\n",
    "\n",
    "boto_session = boto3.Session()\n",
    "sagemaker_session = sagemaker.Session(boto_session=boto_session)\n",
    "sm_client = boto3.client(\"sagemaker\")\n",
    "region = boto_session.region_name\n",
    "bucket = sagemaker_session.default_bucket()\n",
    "role = sagemaker.get_execution_role()\n",
    "\n",
    "experiment_name = 'DEMO-sagemaker-experiments-pipelines'\n",
    "prefix = experiment_name\n",
    "\n",
    "print(f\"bucket: {bucket}\")\n",
    "print(f\"region: {region}\")\n",
    "print(f\"role: {role}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Set up the experiment\n",
    "\n",
    "Amazon SageMaker Experiments have been built for data scientists that are performing different experiments as part of their model development process and want a simple way to organize, track, compare, and evaluate their machine learning experiments. \n",
    "\n",
    "Let’s start first with an overview of Amazon SageMaker Experiments features: \n",
    "\n",
    "* <u>Organize Experiments:</u> Amazon SageMaker Experiments structures experimentation with a first top level entity called experiment that contains a set of trials. Each trial contains a set of steps called trial components. Each trial component is a combination of datasets, algorithms, parameters, and artifacts. You can picture experiments as the top level “folder” for organizing your hypotheses, your trials as the “subfolders” for each group test run, and your trial components as your “files” for each instance of a test run.\n",
    "* <u>Track Experiments:</u> Amazon SageMaker Experiments allows the data scientist to track experiments automatically or manually. Amazon SageMaker Experiments offers the possibility to automatically assign the sagemaker jobs to a trial specifying the <i>experiment_config</i> argument, or to manually call the tracking APIs.\n",
    "* <u>Compare and Evaluate Experiments:</u> The integration of Amazon SageMaker Experiments with Amazon SageMaker Studio makes it easier to produce data visualizations and compare different trials to identify the best combination of hyperparameters."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It is time to create a new <i>experiment</i>. The experiment name needs to be unique within your AWS account and region. Furthermore, let's assign to it a tag. Tagging your experiments adds metadata to your experiments, trials, and trial components, allowing for more fine grained filtering. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from smexperiments.experiment import Experiment\n",
    "from smexperiments.trial import Trial\n",
    "from smexperiments.trial_component import TrialComponent\n",
    "from smexperiments.tracker import Tracker\n",
    "\n",
    "# create the experiment if it doesn't exist\n",
    "try:\n",
    "    demo_experiment = Experiment.load(experiment_name=experiment_name)\n",
    "    print(\"existing experiment loaded\")\n",
    "except Exception as ex:\n",
    "    if \"ResourceNotFound\" in str(ex):\n",
    "        demo_experiment = Experiment.create(\n",
    "            experiment_name=experiment_name,\n",
    "            description = \"Demo experiment\",\n",
    "            tags = [{'Key': 'demo-experiments', 'Value': 'demo1'}]\n",
    "        )\n",
    "        print(\"new experiment created\")\n",
    "    else:\n",
    "        print(f\"Unexpected {ex}=, {type(ex)}\")\n",
    "        print(\"Dont go forward!\")\n",
    "        raise\n",
    "\n",
    "print(demo_experiment.experiment_arn)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Similar considerations we did for experiments also apply to trials. Let's create a new <i>trial</i> for our test associated with the experiment created earlier."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "create_date = time.strftime(\"%Y-%m-%d-%H-%M-%S\")\n",
    "trial_name = f\"xgboost-tuning-{create_date}\"\n",
    "\n",
    "try:\n",
    "    trial = Trial.load(trial_name=trial_name)\n",
    "    print(\"existing trial loaded\")\n",
    "except Exception as ex:\n",
    "    if \"ResourceNotFound\" in str(ex):\n",
    "        trial = Trial.create(\n",
    "            experiment_name=experiment_name,\n",
    "            trial_name=trial_name,\n",
    "            tags = [{'Key': 'demo-experiments', 'Value': 'demo1'}]\n",
    "        )\n",
    "        print(\"new trial created\")\n",
    "    else:\n",
    "        print(f\"Unexpected {ex}=, {type(ex)}\")\n",
    "        print(\"Dont go forward!\")\n",
    "        raise\n",
    "\n",
    "print(trial.trial_arn)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Setup data\n",
    "\n",
    "Download the California housing dataset, and split it into train-validation-test."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.datasets import fetch_california_housing\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "databunch = fetch_california_housing()\n",
    "dataset = np.concatenate((databunch.target.reshape(-1, 1), databunch.data), axis=1)\n",
    "print(f\"Dataset shape = {dataset.shape}\")\n",
    "\n",
    "train, other = train_test_split(dataset, test_size=0.1)\n",
    "validation, test = train_test_split(other, test_size=0.5)\n",
    "\n",
    "print(f\"Train shape = {train.shape}\")\n",
    "print(f\"Validation shape = {validation.shape}\")\n",
    "print(f\"Test shape = {test.shape}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally, upload the datasets to S3."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.savetxt(\"train.csv\", train, delimiter=\",\")\n",
    "np.savetxt(\"validation.csv\", validation, delimiter=\",\")\n",
    "\n",
    "train_prefix = f\"{prefix}/input/train.csv\"\n",
    "s3_input_train = f\"s3://{bucket}/{train_prefix}\"\n",
    "print(s3_input_train)\n",
    "\n",
    "validation_prefix = f\"{prefix}/input/validation.csv\"\n",
    "s3_input_validation = f\"s3://{bucket}/{validation_prefix}\"\n",
    "print(s3_input_validation)\n",
    "\n",
    "s3 = boto3.client(\"s3\")\n",
    "s3.upload_file(\"train.csv\", bucket, train_prefix)\n",
    "s3.upload_file(\"validation.csv\", bucket, validation_prefix)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Set an output path where the trained model will be saved."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "output_path = f's3://{bucket}/{prefix}/output'\n",
    "print(output_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Set up automatic model tuning\n",
    "\n",
    "There are several steps needed configure an automatic tuning job.\n",
    "\n",
    "1/ Retrieve the XGBoost algorithm container."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "xgboost_container = sagemaker.image_uris.retrieve(\"xgboost\", region, \"1.2-2\")\n",
    "print(\"XGBoost container image URI: {}\".format(xgboost_container))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2/ Initialize XGBoost hyperparameters and the XGBoost estimator."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "hyperparameters = {\n",
    "    \"objective\": \"reg:squarederror\",\n",
    "    \"num_round\": \"50\",\n",
    "    \"max_depth\": \"5\",\n",
    "    \"eta\": \"0.2\",\n",
    "    \"gamma\": \"4\",\n",
    "    \"min_child_weight\": \"6\",\n",
    "    \"subsample\": \"0.7\",\n",
    "    \"verbosity\": \"1\"\n",
    "}\n",
    "\n",
    "estimator = sagemaker.estimator.Estimator(image_uri=xgboost_container, \n",
    "                                          hyperparameters=hyperparameters,\n",
    "                                          role=role,\n",
    "                                          instance_count=1, \n",
    "                                          instance_type='ml.m5.2xlarge', \n",
    "                                          volume_size=5, # 5 GB \n",
    "                                          output_path=output_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "3/ Define the hyperparameters values ranges."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sagemaker.tuner import ContinuousParameter, HyperparameterTuner\n",
    "\n",
    "hyperparameter_ranges = {\n",
    "    \"lambda\": ContinuousParameter(0.01, 10, scaling_type=\"Logarithmic\")\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "4/ Define the objective metric we are interested in and whether we are looking to minimize or optimize it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "objective_metric_name = 'validation:rmse'\n",
    "objective_type = 'Minimize'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "5/ Configure the tuning job.\n",
    "\n",
    "<u>NOTE:</u> When using the `Bayesian` strategy, we recommend you to set the parallel jobs value to less than 10% of the total number of training jobs (we will set it higher just for this example to keep it short)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tuner = HyperparameterTuner(estimator,\n",
    "                            objective_metric_name,\n",
    "                            hyperparameter_ranges,\n",
    "                            objective_type=objective_type,\n",
    "                            strategy=\"Bayesian\",\n",
    "                            max_jobs=10,\n",
    "                            max_parallel_jobs=3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "5/ Execute the training jobs with automatic tuning. This will take around ~10-15 minutes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "\n",
    "from sagemaker.inputs import TrainingInput\n",
    "\n",
    "content_type = \"csv\"\n",
    "train_input = TrainingInput(s3_input_train, content_type=content_type)\n",
    "validation_input = TrainingInput(s3_input_validation, content_type=content_type)\n",
    "tuner.fit({'train': train_input, 'validation': validation_input})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Describe the job status."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "boto3.client('sagemaker').describe_hyper_parameter_tuning_job(\n",
    "    HyperParameterTuningJobName=tuner.latest_tuning_job.job_name\n",
    ")['HyperParameterTuningJobStatus']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## SageMaker Experiments APIs\n",
    "\n",
    "Once the tuning job is completed, each training job that was spawned also generated a <i>trial component</i> that is not associated with neither a <i>trial</i> nor an <i>experiment</i>. The Amazon SageMaker Experiments SDK offers filtering capabilities to quickly retrieve the list of trial components we are interested in, and associate them with the intended trial. Let's see how we can do this."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def associate_trial_components(trial_name, search_expression, verbose=True):\n",
    "\n",
    "    # Search iterates over every page of results by default.\n",
    "    trial_component_search_results = list(\n",
    "        TrialComponent.search(search_expression=search_expression)\n",
    "    )\n",
    "    \n",
    "    if verbose:\n",
    "        print(f\"Found {len(trial_component_search_results)} trial components.\")\n",
    "        \n",
    "    # Associate the components with the trial.\n",
    "    for tc in trial_component_search_results:\n",
    "        if verbose:\n",
    "            print(f\"Associating trial component {tc.trial_component_name} with trial {trial.trial_name}.\")\n",
    "        trial.add_trial_component(tc.trial_component_name)\n",
    "        # sleep to avoid throttling\n",
    "        time.sleep(0.5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Define a Search Expression as s boolean conditional statement to combine filters. Then manually associate the trial components to the trial."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from smexperiments.search_expression import Filter, Operator, SearchExpression\n",
    "\n",
    "# return the latest training job name\n",
    "tuning_job_name = tuner.latest_tuning_job.name\n",
    "\n",
    "# The training job names contain the tuning job name, and the training job name is in the source ARN.\n",
    "source_arn_filter = Filter(\n",
    "    name=\"TrialComponentName\", operator=Operator.CONTAINS, value=tuning_job_name\n",
    ")\n",
    "\n",
    "source_type_filter = Filter(\n",
    "    name=\"Source.SourceType\", operator=Operator.EQUALS, value=\"SageMakerTrainingJob\"\n",
    ")\n",
    "\n",
    "search_expression = SearchExpression(\n",
    "    filters=[source_arn_filter, source_type_filter]\n",
    ")\n",
    "\n",
    "associate_trial_components(trial_name, search_expression)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Hosting\n",
    "\n",
    "We deploy the best model to an endpoint. This will take ~5-10 minutes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "\n",
    "from sagemaker.serializers import CSVSerializer\n",
    "\n",
    "tuner_predictor = tuner.deploy(\n",
    "    initial_instance_count=1,\n",
    "    instance_type=\"ml.c5.xlarge\",\n",
    "    serializer=CSVSerializer()\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Predict one test record and compare it with the actual value."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"Predicted:\\t{tuner_predictor.predict(test[0, 1:])}\")\n",
    "print(f\"Actual:\\t\\t{test[0, 0]}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Explore the results of hyperparameter tuning\n",
    "\n",
    "SageMaker offers a very convenient way to retrieve the <i>TrailComponents</i> data. Using the same filtering, we can extract into a `pandas` DataFrame all returned TrialComponents to further analyze the data. For example, we want to see how the hyperapeters has affected the model metric."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sagemaker.analytics import ExperimentAnalytics\n",
    "\n",
    "trial_component_analytics = ExperimentAnalytics(\n",
    "    experiment_name=demo_experiment.experiment_name,\n",
    "    search_expression=search_expression.to_boto()\n",
    ")\n",
    "analytic_table = trial_component_analytics.dataframe()\n",
    "analytic_table.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Plot the last validation dataset RMSE value of each trial component against the lambda the run used. Once you have the data in a `pandas` DataFrame, you can decide your to use your preferred plotting tool, e.g., plots directly from `pandas`, or use `matplotlib` if you want to have more control."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ax = analytic_table.plot.scatter(\"lambda\", \"validation:rmse - Last\", grid=True)\n",
    "analytic_table[\"TrialComponentID\"] = [str(int(x.split('-')[4])) for x in analytic_table[\"TrialComponentName\"]]\n",
    "for _, v in analytic_table[[\"TrialComponentID\", \"lambda\", \"validation:rmse - Last\"]].iterrows():\n",
    "    ax.annotate(v.TrialComponentID, v[1:])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Clean up\n",
    "Delete the endpoint to avoid unnecessary charges."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tuner_predictor.delete_endpoint()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Uncomment the cell below to remove all Experiments, and associated Trials and TrialsComponents."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#demo_experiment.delete_all(action=\"--force\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Next steps\n",
    "\n",
    "Now go to the [SageMaker Pipelines](./02-PipelineExperiments.ipynb) notebook to learn how to package this notebook's workflow in a single pipeline."
   ]
  }
 ],
 "metadata": {
  "instance_type": "ml.t3.medium",
  "kernelspec": {
   "display_name": "Python 3 (Data Science)",
   "language": "python",
   "name": "python3__SAGEMAKER_INTERNAL__arn:aws:sagemaker:eu-west-1:470317259841:image/datascience-1.0"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
